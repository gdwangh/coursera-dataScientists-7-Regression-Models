1. use the residuals (fit$residuals) of our model to estimate the standard deviation (sigma) of the error

sqrt(sum(fit$residuals^2)/(n-2))
= summary(fit)$sigma
= sqrt(deviance(fit)/(n-2))

sum(fit$residuals^2) = deviance(fit)

2. Variation = Residual Variation + Regression Variation
sum of squared term represented Total Variation: Yi-mean(Yi)
sum of squared term represents Residual Variation: Yi-Yi_hat
R^2 = the percent of total variation described by the model, the regression variation

sRes: deviance(fit)  --- Yi-Yi_hat
sTot: sum((galton$child-mean(galton$child))^2) --- Yi-mean(Yi)

the ratio sRes/sTot represents the percent of total variation contributed by the residuals

R^2 = 1 - sRes/sTot
= summary(fit)$r.squared
= cor(galton$child, galton$parent) ^ 2

R^2 is the percentage of variation explained by the regression model. As a percentage it is between 0 and 1. It also equals the sample correlation squared. However, R^2 doesn't tell the whole story.

residuals versus fitted values plot: plot(fit, which=1)
  -- 目的是确认： has none of the patterned appearance of the first. It looks as we would expect if residuals were independently and (almost) identically distributed with zero mean, and were uncorrelated with the fit.
  如果发现有偏离率较高的点，则要将该点从数据集中去掉：The change which inclusion or exclusion of a sample induces in coefficents is a simple measure of its influence. Subtract coef(fitno) from coef(fit) to see the change induced by including the influential first sample.

dfbeta: The function, dfbeta, does the equivalent calculation for every sample in the data. 结果大于一般值的记录，对应图上的偏离点

偏离点y值与排除偏离点以后的回归线预测偏离点y值之间的差距, iss sometimes called influence, sometimes leverage, and sometimes hat value
  out2是包含偏离点的数据，fitno是排除偏离点之后的回归线，1是偏离点：
  resno <- out2[1, "y"] - predict(fitno, out2[1,]) 
 
  1-resid(fit)[1]/resno = hatvalues(fit)[1] --- 是否排除一个点，y值之间的差异大小

hatvalues: The function, hatvalues, performs for every sample a calculation equivalent to the one you've just done. 结果大于一般值的记录，对应图上的偏离点

 standardized residuals：
  the sample standard deviation of fit's residual: deviance(fit)
  the residual degrees of freedom: df.residual(fit)

   sigma <- sqrt(deviance(fit)/df.residual(fit))
   fit's standardized residual: rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
   = rstandard(fit) --- 可以用于看偏离率

   plot(fit, which=3) --- A Scale-Location plot shows the square root of standardized residuals against fitted values

  plot(fit, which=2) -- QQ plot of standardized residuals against normal with constant variance

rstudent(fit): The function, rstudent, calculates Studentized residuals for each sample using a procedure equivalent to that which we just used for the outlier
    sigma1<-sqrt(deviance(fitno)/df.residual(fitno))
    resid(fit)[1]/(sigma1 * sqrt(1-hatvalues(fit)[1]))
   是否删除偏离点的差距。差距很大的就是偏离点


Cook's distance: It is essentially the sum of squared differences between values fitted with and without a particular sample. 
   dy<-predict(fitno, out2) - predict(fit, out2)
   sum(dy^2)/(2*sigma^2)
   = cooks.distance(fit)[1]
  
   对应的图：plot(fit, which=5)


A variance inflation factor (VIF) is a ratio of estimated variances, the variance due to including the ith regressor, divided by that due to including a corresponding ideal
 regressor which is uncorrelated with the others. VIF's show, for each regression coefficient, the variance inflation due to including all the others.
VIF 可用于衡量fit中的各变量相比基准的相关程度，可以从全部变量的fit开始，计算变量的VIF，删除最大的那个变量，得到新的fit. 如果发现某个变量并未由于本次动作见效VIF值，说明这个变量与减小的变量不相关。反复这个过程，直到剩余的变量都不相关。
  mdl<-lm(Fertility~., data=swiss)
  vif(mdl)

VIF is the square of standard error inflation
If a regressor is strongly correlated with others, hence will increase their VIF's, why
shouldn't we just exclude it? --- Excluding it might bias coefficient estimates of regressors with which it is correlated.

The problems of variance inflation and bias due to excluded regressors both involve correlated regressors. However there are methods, such as factor analysis or principal componenent analysis, which can convert regressors to an equivalent uncorrelated set.Why then, when modeling, should we not just use uncorrelated regressors and avoid all the trouble? -- Using converted regressors may make interpretation difficult



Omitting a regressor can bias estimation of the coefficient of certain other regressors.Which ones?
---  Correlated regressors

Including more regressors will reduce a model's residual sum of squares, even if the new regressors are irrelevant. True or False?
--- True

When adding regressors, the reduction in residual sums of squares should be tested for
significance above and beyond that of reducing residual degrees of freedom. R's anova()
function uses an F-test for this purpose. What else should be done to insure that anova() applies?
--- Model residuals should be tested for normality
     shapiro.test(fit3$residuals)

